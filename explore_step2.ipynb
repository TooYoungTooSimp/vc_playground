{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23433ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class LogMelDataset(Dataset):\n",
    "    def __init__(self, logmel, label):\n",
    "        self.logmel = logmel\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.logmel)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.logmel[idx], self.label[idx]\n",
    "\n",
    "\n",
    "dataset = torch.load(\"dataset_slim.pt\", weights_only=False)\n",
    "label_vocab = sorted(set(dataset.label))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(label_vocab)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae26e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    logmel, labels = zip(*batch)\n",
    "    lengths = torch.tensor([f.size(0) for f in logmel])\n",
    "    feats_padded = pad_sequence([*logmel], batch_first=True)  # (B,T,F)\n",
    "    labels = torch.tensor([label_to_idx[l] for l in labels], dtype=torch.long)\n",
    "    return feats_padded, labels, lengths\n",
    "\n",
    "\n",
    "train_ds, val_ds = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35fda6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1 << 14):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe)  # (L, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:T].unsqueeze(0)  # type: ignore\n",
    "\n",
    "\n",
    "class XVector(nn.Module):\n",
    "    def __init__(\n",
    "        self, feat_dim=80, d_model=256, nhead=4, nlayers=4, emb_dim=256, num_classes=100\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(feat_dim, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            enc_layer,\n",
    "            num_layers=nlayers,\n",
    "            enable_nested_tensor=False,\n",
    "        )\n",
    "        self.pos = PosEnc(d_model)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.classify = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (B,T,80)\n",
    "        x = self.pos(self.proj(x))  # (B,T,d_model)\n",
    "        B, T, _ = x.size()\n",
    "        mask = torch.arange(T, device=x.device)[None, :] >= lengths[:, None]  # (B,T)\n",
    "        rev_mask = ~mask\n",
    "        h = self.encoder(x, src_key_padding_mask=mask)  # (B,T,d_model)\n",
    "        # print(h.shape, mask.shape) # torch.Size([8, 697, 256]) torch.Size([8, 697])\n",
    "        rev_mask = rev_mask.unsqueeze(-1)\n",
    "        lengths = lengths.unsqueeze(-1)\n",
    "        mean = (h * rev_mask).sum(1) / lengths\n",
    "        m2 = ((h * h) * rev_mask).sum(1) / lengths\n",
    "        var = (m2 - mean * mean).clamp_min(1e-6)\n",
    "        std = torch.sqrt(var)\n",
    "        pooled = torch.cat([mean, std], dim=1)\n",
    "        emb = self.fc(pooled)  # (B, emb_dim)\n",
    "        logits = self.classify(emb)  # (B, num_classes)\n",
    "        return logits, emb\n",
    "\n",
    "\n",
    "# batch1 = next(iter(train_loader))\n",
    "# logmel, labels, lengths = batch1\n",
    "# model = XVector(num_classes=len(label_vocab))\n",
    "# logits, emb = model(logmel, lengths)\n",
    "# print(logits.shape, emb.shape)\n",
    "num_classes = len(label_vocab)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = XVector(\n",
    "    feat_dim=80,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    nlayers=3,\n",
    "    emb_dim=256,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2652f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4539f1e46fad4a2fb62bb70a0d0519ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68021d6e1a074135a7611fccbbce1fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation accuracy: 34.28%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2339141d064f3aad8da77153406b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m loss = crit(logits, labs)\n\u001b[32m     12\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m loss.backward()\n\u001b[32m     14\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m5.0\u001b[39m)\n\u001b[32m     15\u001b[39m opt.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Applications\\Python\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m torch.autograd.backward(\n\u001b[32m    649\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    650\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Applications\\Python\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m _engine_run_backward(\n\u001b[32m    354\u001b[39m     tensors,\n\u001b[32m    355\u001b[39m     grad_tensors_,\n\u001b[32m    356\u001b[39m     retain_graph,\n\u001b[32m    357\u001b[39m     create_graph,\n\u001b[32m    358\u001b[39m     inputs,\n\u001b[32m    359\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    360\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Applications\\Python\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    825\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    826\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import deque\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total, seen = 0.0, 0\n",
    "    losses = deque(maxlen=100)\n",
    "    for feats, labs, lengths in (pb := tqdm(train_loader, leave=False)):\n",
    "        feats, labs, lengths = feats.to(device), labs.to(device), lengths.to(device)\n",
    "        logits, emb = model(feats, lengths)\n",
    "        loss = crit(logits, labs)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "        total += loss.item() * feats.size(0)\n",
    "        seen += feats.size(0)\n",
    "        losses.append(loss.item())\n",
    "        pb.set_postfix(loss=np.mean(losses))\n",
    "    sched.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total, correct = 0, 0\n",
    "        for feats, labs, lengths in tqdm(val_loader, leave=False):\n",
    "            feats, labs, lengths = feats.to(device), labs.to(device), lengths.to(device)\n",
    "            logits, emb = model(feats, lengths)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} Validation accuracy: {acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
