{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d70437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from ba_datasets import LogMelDataset, load_dataset\n",
    "from models import XVector\n",
    "from tools import *\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655af058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "hubert_name = \"facebook/hubert-large-ls960-ft\"\n",
    "hubert = HubertModel.from_pretrained(hubert_name).to(device).eval()  # type: ignore\n",
    "hubert_fe = Wav2Vec2FeatureExtractor.from_pretrained(hubert_name)\n",
    "hubert_sr = 16000\n",
    "\n",
    "centroids = pd.read_parquet(\"xvector_centroids.parquet\")\n",
    "centroids_dict = {x[\"label\"]: x[\"emb\"] for x in centroids.to_dict(\"records\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1636aa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/work/ba_voice/.venv/lib/python3.13/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/home/lyc/work/ba_voice/.venv/lib/python3.13/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([114, 1024]) torch.Size([1, 114]) torch.Size([1, 114, 80])\n",
      "cpu cpu cpu\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def to_16ms(x, target_length):\n",
    "    # scale time dimension by 1.25 (50 Hz -> 62.5 Hz)\n",
    "    x_up = F.interpolate(\n",
    "        x.transpose(-1, -2),\n",
    "        scale_factor=(target_length + 1) / x.shape[-2],\n",
    "        mode=\"linear\",\n",
    "        align_corners=False,\n",
    "    )[:, :, :target_length]\n",
    "    return x_up.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def prepare_wav(wav, sr):\n",
    "    wav = wav.to(device)\n",
    "    if wav.ndim > 1:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)  # to mono\n",
    "    if sr != hubert_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, hubert_sr)\n",
    "        sr = hubert_sr\n",
    "    logmel = to_logmelspec(wav, sr)\n",
    "    f0 = extract_f0_torchcrepe(wav)\n",
    "    # plt.plot(f0.cpu().numpy()[0])\n",
    "\n",
    "    # HuBERT runs at ~20ms stride; weâ€™ll resample to 16ms by linear interpolation later\n",
    "    inputs = hubert_fe(\n",
    "        wav.cpu().numpy(), sampling_rate=hubert_sr, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        out = hubert(inputs.input_values.to(device))  # last hidden state (B,T_h,D)\n",
    "        H = out.last_hidden_state.squeeze(0)\n",
    "\n",
    "    H = to_16ms(H.unsqueeze(0), f0.shape[-1]).squeeze(0)  # (T_h,D)\n",
    "    H, f0, logmel = H.cpu(), f0.cpu(), logmel.cpu()\n",
    "    return H, f0, logmel\n",
    "\n",
    "\n",
    "audio_files = sorted(glob(\"VOC_JP/**/*.ogg\", recursive=True))\n",
    "audio_labels = [os.path.basename(os.path.dirname(f)) for f in audio_files]\n",
    "wav, sr = torchaudio.load(audio_files[0])\n",
    "H, f0, logmel = prepare_wav(wav, sr)\n",
    "print(H.shape, f0.shape, logmel.shape)\n",
    "print(H.device, f0.device, logmel.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Content2MelAttn\n",
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "vocoder.to(device).eval()  # type: ignore\n",
    "\n",
    "model_vc = Content2MelAttn(\n",
    "    content_dim=1024,\n",
    "    spk_dim=256,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    nlayers=16,\n",
    "    ff_mult=8,\n",
    "    n_mels=80,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "model_vc.eval()\n",
    "\n",
    "\n",
    "def convert_voice(wav_path: str, target_spk: str):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = torch.mean(wav, dim=0, keepdim=True)  # to mono\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    sr = 16000\n",
    "\n",
    "    H, f0, _ = prepare_wav(wav, sr)  # we won't use source mel here\n",
    "    V = centroids_dict[target_spk]  # your centroid, (256,)\n",
    "    V = torch.tensor(np.array(V))\n",
    "    print(H.shape, f0.shape, V.shape)\n",
    "    with torch.no_grad():\n",
    "        pred_mel = model_vc(\n",
    "            H.unsqueeze(0).to(device),\n",
    "            f0.to(device),\n",
    "            V.unsqueeze(0).to(device),\n",
    "        )[\n",
    "            0\n",
    "        ]  # (T,80)\n",
    "        y = vocoder(pred_mel.unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "    return y, pred_mel.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC_JP/JP_Airi/airi_cafe_act_1.ogg -> JP_Yuuka\n",
      "torch.Size([203, 1024]) torch.Size([203]) torch.Size([256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/work/ba_voice/.venv/lib/python3.13/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/home/lyc/work/ba_voice/.venv/lib/python3.13/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (203) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m rnd = \u001b[32m19160\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(audio_files[idx], \u001b[33m\"\u001b[39m\u001b[33m->\u001b[39m\u001b[33m\"\u001b[39m, audio_labels[rnd])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m y, mel = \u001b[43mconvert_voice\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m y2, mel2 = convert_voice(audio_files[idx], audio_labels[rnd])\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# plt.imshow(mel.transpose(0, 1), aspect=\"auto\", origin=\"lower\")\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# plt.imshow(mel2.transpose(0, 1), aspect=\"auto\", origin=\"lower\")\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# plot mel and mel2 together\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mconvert_voice\u001b[39m\u001b[34m(wav_path, target_spk)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(H.shape, f0.shape, V.shape)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     pred_mel = \u001b[43mmodel_vc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mV\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m     38\u001b[39m         \u001b[32m0\u001b[39m\n\u001b[32m     39\u001b[39m     ]  \u001b[38;5;66;03m# (T,80)\u001b[39;00m\n\u001b[32m     40\u001b[39m     y = vocoder(pred_mel.unsqueeze(\u001b[32m0\u001b[39m)).squeeze(\u001b[32m0\u001b[39m).cpu().numpy()\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y, pred_mel.cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/ba_voice/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/ba_voice/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/ba_voice/model_Content2MelAttn.py:114\u001b[39m, in \u001b[36mContent2MelAttn.forward\u001b[39m\u001b[34m(self, H, f0, spk, lengths)\u001b[39m\n\u001b[32m    112\u001b[39m x = torch.cat([H, f0.unsqueeze(-\u001b[32m1\u001b[39m)], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B,T, content+1)\u001b[39;00m\n\u001b[32m    113\u001b[39m x = \u001b[38;5;28mself\u001b[39m.inp(x)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# build padding mask if lengths provided (True = pad)\u001b[39;00m\n\u001b[32m    117\u001b[39m key_padding_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/ba_voice/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/ba_voice/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/ba_voice/model_Content2MelAttn.py:18\u001b[39m, in \u001b[36mPosEnc.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):  \u001b[38;5;66;03m# x: (B,T,D)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (203) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model_vc.load_state_dict(torch.load(\"content2mel_selfrec.pt\", map_location=device))\n",
    "\n",
    "sorted(audio_files)\n",
    "audio_labels[0]\n",
    "import random\n",
    "\n",
    "idx = 1\n",
    "rnd = random.randint(0, len(audio_files) - 1)\n",
    "rnd = 19160\n",
    "print(audio_files[idx], \"->\", audio_labels[rnd])\n",
    "y, mel = convert_voice(audio_files[idx], audio_labels[idx])\n",
    "y2, mel2 = convert_voice(audio_files[idx], audio_labels[rnd])\n",
    "# plt.imshow(mel.transpose(0, 1), aspect=\"auto\", origin=\"lower\")\n",
    "# plt.imshow(mel2.transpose(0, 1), aspect=\"auto\", origin=\"lower\")\n",
    "# plot mel and mel2 together\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mel.transpose(0, 1), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Original Mel\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mel2.transpose(0, 1), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Random Mel\")\n",
    "plt.show()\n",
    "\n",
    "display_audio(audio_files[idx], 44100)\n",
    "display_audio(y, 16000)\n",
    "display_audio(y2, 16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
